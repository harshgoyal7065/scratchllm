{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlrGbycmhH5+DN6qvVRS0Q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We will be building our own LLM from scratch, because we can! Now, this notebook is aimed to be a step-by-step guide, but who knows, I might get tired and then its just a bunch of random code that nobody understands. Anyways, let's start.\n",
        "\n",
        "First thing we build is a tokenizer. Think of tokens as the smallest unit our model can interact with. For more simplicity, it can be thought as a word in a sentence (I know that character is the smallest in a sentence, but you get the point)\n",
        "\n",
        "The simplest tokenizer one can think of is converting each character to its ASCII value. So,\n",
        "\n",
        " ```tokenize(\"ace\") = [97,99,101] and similarly deTokenize([97,99,101]) = \"ace\"```\n",
        "\n",
        "\n",
        "Tokenizer converts out input (in this case, text) into tokens. So essentially this is a funtion that takes input and returns an array of integer (our token is mapped by integer values)\n",
        "So, let's get into it. I will try this guide to be as extensive as possible.\n",
        "\n",
        "Dataset - Take any book of your liking which is available publically, for now, I have taken this shakespeare gist (https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt) for data."
      ],
      "metadata": {
        "id": "uOgFUizCydXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(\"/content/sample_data\", exist_ok=True)\n",
        "\n",
        "file_path = \"/content/sample_data/shakespeare.txt\"\n",
        "\n",
        "# Download the file if it doesn't exist\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {os.path.basename(file_path)}...\")\n",
        "    !wget -q -O {file_path} https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Read the file and store the content in raw_text\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tD3xL26I9ubA",
        "outputId": "eec3d93b-fdd3-40d1-984c-ae05a9958ac7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading shakespeare.txt...\n",
            "Download complete.\n",
            "Total number of character: 5436475\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re # For Regex\n",
        "\n",
        "# Okay, so we are now implementing our tokenizer\n",
        "# Step 1 - Split text the by special characters, that's exactly what re.split is doing\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "# For the purpose of our example, we will be removing all the whitespaces (this is a decision to make depending on the case, for instance, spaces are important for python code, so we cant ignore them if the dataset had python code)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()] #Pre Processed has all the splitted words\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFlNySjEFxlp",
        "outputId": "3c327116-aab2-4d21-a934-061ae078399e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['From', 'fairest', 'creatures', 'we', 'desire', 'increase', ',', 'That', 'thereby', 'beauty', \"'\", 's', 'rose', 'might', 'never', 'die', ',', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',', 'His', 'tender', 'heir', 'might']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have splitted the words for our tokenizer, its time to map them to a integer value.\n",
        "\n",
        "For simplicity, we will just sort the list in preprocessed and give the index as an integer\n",
        "\n",
        "This sorted array will essentially create the mapping of each splitted word (or token) to exact input/human form. We will call it vocabolary\n",
        "\n",
        "Note that both splitting by word and vocabolary creation can differ for each commercial model. We are just trying to get the intuition of the flow, rather than the exact functions being used"
      ],
      "metadata": {
        "id": "VlqYnFVizBFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed)) #set to remove duplicate words\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-t8W2_AHoCC",
        "outputId": "bba6e54d-4712-43a1-f598-b651f944df68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)} #Create vocabolary in hashmap/dict. Though all_words array should have worked but making it a doct give us O(1) lookup"
      ],
      "metadata": {
        "id": "VPcxA8pqH9O-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to both encode and decode the tokens from token id in the future, so we will create a tokenizer class that gives us both the function\n",
        "\n",
        "Encoding will involve getting map from vocab\n",
        "\n",
        "For decoding, we will create an inverse of vocab where key is value and value is key, for faster lookup"
      ],
      "metadata": {
        "id": "8BfZL45ZzSCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV1:\n",
        "    # initialize Tokenizer class with the vocabolary. This tokenizer class can be used for different programs with different vocab\n",
        "    def __init__(self, vocab):\n",
        "        self.word_to_id_dict = vocab # Map used for encoding\n",
        "        self.id_to_word_dict = {i:s for s,i in vocab.items()} # Reverse map, used for decoding\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.word_to_id_dict[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.id_to_word_dict[i] for i in ids]) # Join all the characters in the ids through spaces\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Remove spaces from front of special characters\n",
        "        return text"
      ],
      "metadata": {
        "id": "QPgwQftYheXo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDCeru9tjwFC",
        "outputId": "67acaebf-6e38-4fd7-80f1-240f70bd019f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3516, 17246, 14371, 33301, 15157, 20576, 8, 8124, 30894, 11200, 5, 27669, 27515, 22889, 23644, 15292, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3dnwdzhGkXTG",
        "outputId": "57c7ee33-b8fe-41a5-d278-4f7a2d6b7bd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" From fairest creatures we desire increase, That thereby beauty\\' s rose might never die,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what if we encounter a word that is not there in the document? Then our program will fail. To handle those cases, we will use 2 special tokens\n",
        "1. <|unk|> for \"unknown\" words and\n",
        "2. <|endoftext|> for \"end of file\"\n",
        "endoftext is needed since LLMs are trained on mulitple data sources, so it helps LLM to differentiate in the sources.\n",
        "\n",
        "For our updated tokenizer, we need to add the two tokens in the vocab first"
      ],
      "metadata": {
        "id": "imEInYC4zc45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "all_words.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeVErXTWkt8H",
        "outputId": "bd1b2c03-7fdd-4216-a6cb-6fb0e41cd97a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34193"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the first intuition might be to just update the vocab and it will auto-handle the new token, but its not, we need to make a few changes to encode and decode\n",
        "function as well. I am going to write a new version `TokenizerV2` for this, but feel free to continue in the previous one as well"
      ],
      "metadata": {
        "id": "d2j2j6-dzh5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.word_to_id_dict = vocab # Updated Map used for encoding\n",
        "        self.id_to_word_dict = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        # We have created the item array, hence, here is where unknown should come\n",
        "        preprocessed = [\n",
        "            item if item in self.word_to_id_dict\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.word_to_id_dict[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.id_to_word_dict[i] for i in ids]) # Join all the characters in the ids through spaces\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Remove spaces from front of special characters\n",
        "        return text"
      ],
      "metadata": {
        "id": "2OvjlM63lymM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWLdZjHvmaN0",
        "outputId": "f9db55b5-d3be-4ca8-df09-640427826a4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXKCx_5tmhna",
        "outputId": "87e84d83-7c8f-4da6-a698-050d9ff7d78c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34191,\n",
              " 8,\n",
              " 15758,\n",
              " 34147,\n",
              " 21881,\n",
              " 34191,\n",
              " 262,\n",
              " 34192,\n",
              " 4320,\n",
              " 30855,\n",
              " 34191,\n",
              " 34191,\n",
              " 24067,\n",
              " 30855,\n",
              " 24572,\n",
              " 76]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "186ihOC3mttK",
        "outputId": "a1e3d5a8-9c9d-4be2-ab1c-c9a02e8c0bc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like <|unk|>? <|endoftext|> In the <|unk|> <|unk|> of the palace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This unknown and endoftext token is called Special Context Token and GPT uses endoftext for its working\n",
        "Other Special Context Token includes\n",
        "\n",
        "1. **[BOS] (beginning of sequence)**: This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "2. **[EOS] (end of sequence)**: This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
        "\n",
        "3. **[PAD] (padding)**: When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch.\n",
        "\n",
        "We will not worry about these for now, but you get the idea! You can add this in your implementation"
      ],
      "metadata": {
        "id": "uZX7KwEkzl1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw a simple word-based tokenization, where each word is a token, but the downside of this is we have to have a vocab with all the words as separate token\n",
        "Now this might seem logical at first, it will create problems like Large Vocab needed and a word being out of vocabolary.\n",
        "\n",
        "Also, we lose the relation between different words. For instance, assume a sentence \"small smaller light lighter big bigger\", noe if we use word based tokenization We need 6 tokens (each for a word) and those will be completely different.\n",
        "\n",
        "Hence, to solve this problem, we use sub word based tokenization. (There is a character based tokenization as well, but that has its own challenges, you can read about them)\n",
        "\n",
        "**What we will implement is a Byte Pair Encoder.**\n",
        "\n",
        "a. Paper for BPE - https://www.derczynski.com/papers/archive/BPE_Gage.pdf\n",
        "\n",
        "b. GPT uses tiktoken as its encoder, so you can directly use that as well (https://github.com/openai/tiktoken)\n",
        "\n",
        "We will use titoken as our BPE tokenizer for this notebook, but we implement our own BPE tokenizer as well which can be found here. So, chose whatever you want to do."
      ],
      "metadata": {
        "id": "oEVXKQ92zzyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxBsKyCNvkJX",
        "outputId": "7d65806f-6521-41c8-c195-55085d8917cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\") # This was our tokenizer = TokenizerV2(vocab)"
      ],
      "metadata": {
        "id": "HR1SBOXMwxoW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVqrknzBxiPm",
        "outputId": "e36c1ab5-49e6-4929-9819-1009362c999f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our encoded data, we will create input-target pairs or input-output pairs. In simpler words, LLMs dont need to be fed a lot of different data to find input-target.\n",
        "\n",
        "If a sentence is \"I am learning LLM from scratch\", so we can train data like this:-\n",
        "\n",
        "a. Pass 1: \"I\" -> \"am\" (target)\n",
        "\n",
        "b. Pass 2: \"I am\" -> \"learning\" (target)\n",
        "\n",
        "c. Pass 3: \"I am learning\" -> \"LLM\" (target)\n",
        "\n",
        "Did you notice how previous target upon adding with input becomes input for the new target/output. This is called auto-regressive technique."
      ],
      "metadata": {
        "id": "695Y5sYvJf8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = tokenizer.encode(raw_text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIbSQ7x1Jwen",
        "outputId": "6111bf60-85cb-49cd-b376-c26664fd5934"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1836425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:"
      ],
      "metadata": {
        "id": "KI1T-czsL98L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 # length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = encoded_text[:i]\n",
        "    desired = encoded_text[i]\n",
        "\n",
        "    print(context, \"---->\", desired)\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = encoded_text[:i]\n",
        "    desired = encoded_text[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arpQG2HpL7cx",
        "outputId": "9a3563ab-bb5f-4706-8530-b2b7f5e62771"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220] ----> 3574\n",
            "[220, 3574] ----> 37063\n",
            "[220, 3574, 37063] ----> 301\n",
            "[220, 3574, 37063, 301] ----> 8109\n",
            "  ---->  From\n",
            "  From ---->  faire\n",
            "  From faire ----> st\n",
            "  From fairest ---->  creatures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have created input-target pairs that will be helpful in training LLMs. But this is currently just a list. We will need to do it in structured order that will be helpful to us in future as well.\n",
        "\n",
        "So, we will create something called Data Loader. Data loader iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
        "\n",
        "Our data loader will generate 2 tensors\n",
        "1. Input tensor for LLM data to see\n",
        "2. Target tensor to tell LLM about the target character\n",
        "\n",
        "We will use sliding window approach to implement this. This is what we will do\n",
        "\n",
        "1. We will create an input tensor (assume 2D array for now), where each row is the list of token of `context_length` length.\n",
        "  \n",
        "    For instance if token list after encoding is [1,2,3,4,5,6,7,8] and `context_length` is 2 `input_tensor = [[1,2],[3,4],[5,6],[7,8]]`\n",
        "\n",
        "2. We will create an output tensor, where each output row is same as input row moved ahead by 1 index.\n",
        "\n",
        "     For the above `input_tensor`, `output_tensor = [[2,3],[4,5],[6,7],[8]]`"
      ],
      "metadata": {
        "id": "jut2its3OQxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Algorithm\n",
        "\n",
        "Step 1: Tokenize the entire text\n",
        "    \n",
        "Step 2: Use a sliding window to chunk the book into sequences of max_length\n",
        "\n",
        "Step 3: Return the total number of rows in the dataset\n",
        "\n",
        "Step 4: Return a single row from the dataset"
      ],
      "metadata": {
        "id": "S6WwysRqRK1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into sequences of max_length\n",
        "        i = 0\n",
        "        while i < len(token_ids) - max_length:\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "            i+=stride\n",
        "\n",
        "    # Needed for pytorch dataloader\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    # Needed for pytorch dataloader\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "FvyjsFxuOi5K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A"
      ],
      "metadata": {
        "id": "PXUw5nI7UjNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "YQP9wdgnU3Fx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function"
      ],
      "metadata": {
        "id": "RyGcp1jpQTpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nuLIZ0sQV0v",
        "outputId": "56b4b6cd-7605-4b97-c075-4093bc1557e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "[tensor([[  220,  3574, 37063,   301]]), tensor([[ 3574, 37063,   301,  8109]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have out DataLoader, we can convert our tensor into vector embeddings to be served to the LLM. We will be converting our tensors to vector embeddings now.\n",
        "\n",
        "\n",
        "Let's have an intuition behind conversion to vector embedding. Let's say you want to generate a sentence \"I am learning LLM from scratch\"\n",
        "\n",
        "This is in form of words, but computers needs integers to work upon. So we convert sentences into token ids as seen above. But why cant be use these token IDs directly?\n",
        "\n",
        "When we convert the words or subwords into tokens, we lose the relation between them. For instance \"cat\" and \"kitten\" are related, but if we just pass the token ids, we can't fiure out if they are related.\n",
        "\n",
        "So, to do this, we will generate vector corresposnding to some features in a vector (or matrices) format. This is called vector embedding. For instance, if the features I decide to take are [\"is_pet\", \"can_fly\"], I can convert \"dog\" into a vector [90, 2]. So, we can group all words with a high value of \"is_pet\" togther and similarly for \"can_fly\".\n",
        "\n",
        "There are multiple pre-trained vector embedding datasets available online that you can use, or you can train one of your own. One such dataset is [Google's word to vector](https://huggingface.co/fse/word2vec-google-news-300). You can use this if you don't feel like training your own vector embedding.\n",
        "\n",
        "The next block will be an illustration on how to use this dataset, you can skip, if you are trying to train one on your own."
      ],
      "metadata": {
        "id": "dOcVBkPGQ0VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KOsu_E5wQqDD",
        "outputId": "4e5e0a29-cde3-4b62-be59-11d7fcb508d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model\n",
        "print(word_vectors['computer'])\n",
        "\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)) #Find most similar word to king + woman - man which should be something around 'queen'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xfeoZ0uFVPUf",
        "outputId": "dba9b2c0-7844-446e-e2a7-24192b5bdfe9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
            " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
            "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
            "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
            " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
            " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
            "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
            "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
            " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
            " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
            " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
            "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
            " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
            "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
            "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
            "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
            " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
            " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
            "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
            "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
            "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
            "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
            " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
            " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
            "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
            "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
            " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
            "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
            " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
            " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
            " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
            " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
            "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
            " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
            "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
            "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
            " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
            " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
            " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
            " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
            " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
            " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
            " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
            " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
            "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
            " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
            "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
            "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
            " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
            "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
            " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
            "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
            " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
            " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
            " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
            " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
            " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
            " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
            " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
            "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
            "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
            " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
            "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
            " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
            "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
            " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
            "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
            " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
            " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
            " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
            "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
            "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
            "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
            "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
            "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n",
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, to create a vector embedding, we need 2 things - number of token ids and number of features for a vector.\n",
        "\n",
        "For instance, GPT model was trained on 50257 tokens and 768 features, so this can be thought as a 2D array, where each row corresponds to one of the 50257 token and each column corresponds to its value with 768 features.\n",
        "\n",
        "How to generate vector embeddings?\n",
        "1. Assign a random value to the vector values\n",
        "2. Optimize embedding weights as the LLM is trained. This means that with LLM training, vector embedding are also getting trained."
      ],
      "metadata": {
        "id": "_mN0ggXmXtjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will not train out embedding for 768 features like GPT, we will for now, just take 3 features as our training process.\n",
        "# Now, we can take the input tensor generated above to train, or we can use a simple smaller size of let's say 6 words to train.\n",
        "# The issue with training the above input tensor of shakespeare text is that we might not be able to find all the features to divide each word perfectly,\n",
        "# hence, I will be using 6 words and 3 features for now. However, if you want to do it full fleged, you can check the parameters used by Google tensor and\n",
        "# train for 300 or even 768 tokens. Remember that the more the parameters, more energy and resources it will take to train\n",
        "\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "vocab_size = 6 # sixe of vocabolary\n",
        "output_dim = 3 # number of features\n",
        "\n",
        "torch.manual_seed(123) # Random number to hash\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # Generates vector embedding with random values.\n",
        "\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpgPu90AYBqt",
        "outputId": "1f50f0fb-88e5-4531-b642-24bf2cd412ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that weight of embedding layer is 6X3 matrix as we wanted. This acts as a lookup table, so we can lookup any token now."
      ],
      "metadata": {
        "id": "tn6lBf0obvz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))\n",
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxPVfy5fcKV4",
        "outputId": "35d961ad-b6e1-4ee0-8732-6720bfe8d4f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I know, its getting tiring now, but if you are still reading this, we are just about to end our data pre-processing part. (You thought OpenAI is burning money in nothing?). Anyways, we have got our words to have meaning, but what if I have 2 sentences:-\n",
        "\n",
        "1. I am learning LLM from scratch\n",
        "2. LLM is learnt by me from scratch\n",
        "\n",
        "You can see LLM is repeated in both the sentences but its position changes its semantics. But if we embed both the sentences, the vector embedding for LLM will be the same, though they could serve different purposes. Hence, we will now add position to our vector embeddings.\n",
        "\n",
        "There are 2 ways to do it:-\n",
        "1. Absolute position - We give a fixed embedding to a particular position of the sentence and then add the two vectors to find the new vector embedding. Positional vector will have same dimension as original token embedding.\n",
        "\n",
        "2. Relative postion - We check how far apart are 2 words. This way, we ensure that even if sentence length is different from that of training set, model can manage it.\n",
        "\n",
        "Commercially, Absolute position works just fine for most models."
      ],
      "metadata": {
        "id": "Sm0GE0SJeDxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # Create a embedding, this time of a comparable magnitude to GPT"
      ],
      "metadata": {
        "id": "SX77JH0yeXuP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate input_ids with dataloader\n",
        "\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "el6rcms-prfr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2pXpMmcqAJg",
        "outputId": "0beaf514-70c8-4b47-8788-3148f8c3deb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  220,  3574, 37063,   301],\n",
            "        [ 8109,   356,  6227,  2620],\n",
            "        [   11,   198,   220,  1320],\n",
            "        [12839,  8737,   338,  8278],\n",
            "        [ 1244,  1239,  4656,    11],\n",
            "        [  198,   220,   887,   355],\n",
            "        [  262,   374,  9346,   815],\n",
            "        [  416,   640, 12738,   589]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remmber that token embedding layer was somethig like this [[//256 values], [//256 values]], so we extract corresponding 256 values and make a 3D matrix\n",
        "\n",
        "token_embeddings = token_embedding_layer(inputs) # create embedding for each token id\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03LQYkpyqWkV",
        "outputId": "e2127185-d662-4e88-e3e1-6cf0117c1d3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets create positional embedding. At any given point, we have to process at max `context_length` tokens, so we can create token embedding as a `context_length` X `output_dim` vector. Where each row will show the position based value for a particular feature."
      ],
      "metadata": {
        "id": "8q4wlFMxrorX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5zhudHvs7o3",
        "outputId": "07027ab7-12fc-48ca-b025-b618c07ccf14"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we are done, we are finally done with the data pre-processing that can be sent to LLM to train. We will continue with LLM training as we move ahead, but for a moment, step back, make a mind map of what we have done till now. Its a lot for data pre processing.\n",
        "\n",
        "If you are following till now, it means you are having fun. And if you are having fun, its important that you get what we have done till now. And yeah, as I am writing this, I haven't implemented my own BPE tokenizer yet, but it will update as soon as I do that. So watch out! Anyways, onto LLM training.\n",
        "\n",
        "Here's a small recap of the steps done till now:-\n",
        "\n",
        "1. Tokenization (using BPE agorithm and tiktoken)\n",
        "2. Create Input Target pairs\n",
        "3. Creating Token embeddings\n",
        "4. Creating Positional embeddings\n",
        "5. Create Input Embeddings = Token + Positional"
      ],
      "metadata": {
        "id": "jvJLBmiBtE3T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ur1vFKfps86E"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}